{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217795d1-bcc5-4037-9b70-ba9a14632648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as opt\n",
    "import sys\n",
    "import utils\n",
    "import hyp\n",
    "from utils import PrioritizedReplayBuffer \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED, AllChem, rdFingerprintGenerator, Crippen, Descriptors, MACCSkeys\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "from rdkit.Contrib.SA_Score import sascorer\n",
    "from environment import Molecule\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd1b755-3d75-4c7d-af83-ccfedf889807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_pdbqt(smiles: str, output_file: str = \"./docking/ligant.pdbqt\"):\n",
    "    \"\"\"Конвертирует SMILES в PDBQT через Open Babel.\"\"\"\n",
    "    # Создание молекулы из SMILES и добавление водородов\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    mol = Chem.AddHs(mol)\n",
    "    status = AllChem.EmbedMolecule(mol)\n",
    "    if status == -1:\n",
    "        raise RuntimeError(\"Не удалось сгенерировать 3D-структуру\")\n",
    "    \n",
    "    # Сохранение во временный файл .mol\n",
    "    temp_mol = \"./docking/temp.mol\"\n",
    "    Chem.MolToMolFile(mol, temp_mol)\n",
    "    if not os.path.exists(temp_mol):\n",
    "        raise FileNotFoundError(\"Временный файл не создан\")\n",
    "    \n",
    "    # Конвертация в PDBQT через Open Babel\n",
    "    cmd = f\"obabel {temp_mol} -O {output_file}\"\n",
    "    result = subprocess.run(\n",
    "        cmd, \n",
    "        shell=True, \n",
    "        capture_output=True, \n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        error_msg = f\"Ошибка Open Babel:\\n{result.stderr}\"\n",
    "        if \"Invalid output format\" in result.stderr:\n",
    "            error_msg += \"\\nУбедитесь, что Open Babel установлен и добавлен в PATH\"\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "    \n",
    "    if not os.path.exists(output_file):\n",
    "            raise FileNotFoundError(f\"Файл {output_file} не создан\")\n",
    "    \n",
    "    #os.remove(temp_mol)\n",
    "\n",
    "def run_vina_docking(protein_pdbqt: str, ligand_pdbqt: str, center: tuple = (5 , 15, 50), size: tuple = ( 20, 20, 20)) -> float:\n",
    "    \"\"\"Запускает докинг и возвращает энергию связывания.\"\"\"\n",
    "    # Создание конфигурационного файла для Vina\n",
    "    config = f\"\"\"\n",
    "    receptor = {protein_pdbqt}\n",
    "    ligand = {ligand_pdbqt}\n",
    "    out = result.pdbqt\n",
    "    center_x = {center[0]}\n",
    "    center_y = {center[1]}\n",
    "    center_z = {center[2]}\n",
    "    size_x = {size[0]}\n",
    "    size_y = {size[1]}\n",
    "    size_z = {size[2]}\n",
    "    exhaustiveness = 16\n",
    "    cpu = 12\n",
    "    \"\"\"\n",
    "    with open(\"./docking/config.txt\", \"w\") as conf_file:\n",
    "        conf_file.write(config)\n",
    "    # Запуск AutoDock Vina\n",
    "    with open(\"./docking/log.txt\", \"w\") as log_file:\n",
    "        result = subprocess.run(\n",
    "            f\"vina --config ./docking/config.txt\",\n",
    "            stdout=log_file,\n",
    "            text=True,\n",
    "            check=True,\n",
    "            shell=True\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(\"Ошибка Vina:\", result.stderr)\n",
    "            return None\n",
    "    os.remove(\"result.pdbqt\")\n",
    "    # Извлечение энергии связывания из лога\n",
    "    with open(\"./docking/log.txt\", \"r\") as f:\n",
    "        log = f.read()\n",
    "        affinity_values = []\n",
    "        for line in log.split(\"\\n\"):\n",
    "            if line.strip().startswith(\"1\"):  # Первая строка с результатами\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    try:\n",
    "                        affinity = float(parts[1])\n",
    "                        affinity_values.append(affinity)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "        # Возвращаем лучшую энергию\n",
    "        if affinity_values:\n",
    "            return min(affinity_values)\n",
    "        else:\n",
    "            print(\"Энергии связывания не найдены\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3221626-76fc-4756-8ede-f3cfdf56d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_to_pdbqt(\"[H]C(N)c1c(N(C)C#N)nc2[nH]cc(N=O)c2c1-c1cc2c3c(n1)C(CN)(CN)C2(N=N)NC3=O\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73849715-53a7-494f-802d-6fa09da13c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_vina_docking(\"./docking/GSK3-b.pdb\", \"./docking/ligant.pdbqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9eb917-ae9f-445b-8e15-2ada11f207ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy Linear Layer for exploration\"\"\"\n",
    "    def __init__(self, in_features, out_features, std_init=0.4):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        \n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        \n",
    "        self.register_buffer('weight_epsilon', torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer('bias_epsilon', torch.Tensor(out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / np.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "        \n",
    "        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "            return F.linear(x, weight, bias)\n",
    "        else:\n",
    "            return F.linear(x, self.weight_mu, self.bias_mu)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scale_noise(size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "\n",
    "class RainbowDQN(nn.Module):\n",
    "    \"\"\"Rainbow DQN Network with Dueling Architecture and Distributional RL\"\"\"\n",
    "    def __init__(self, input_length, output_length, atoms=51, v_min=-10, v_max=10):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "        self.atoms = atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.output_length = output_length\n",
    "        \n",
    "        # Feature extraction\n",
    "        self.linear_1 = NoisyLinear(input_length, 1024)\n",
    "        self.linear_2 = NoisyLinear(1024, 512)\n",
    "        \n",
    "        # Dueling streams\n",
    "        self.value_stream = nn.Sequential(\n",
    "            NoisyLinear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(128, atoms)\n",
    "        )\n",
    "        \n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            NoisyLinear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(128, output_length * atoms)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        \n",
    "        value = self.value_stream(x).view(-1, 1, self.atoms)\n",
    "        advantage = self.advantage_stream(x).view(-1, self.output_length, self.atoms)\n",
    "        \n",
    "        q_dist = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        return F.softmax(q_dist, dim=2)\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, NoisyLinear):\n",
    "                module.reset_noise()\n",
    "    \n",
    "    def get_q_values(self, x):\n",
    "        with torch.no_grad():\n",
    "            dist = self.forward(x)\n",
    "            support = torch.linspace(self.v_min, self.v_max, self.atoms).to(x.device)\n",
    "            q_values = (dist * support).sum(dim=2)\n",
    "            return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d5eb6d5-93fe-48f4-a4a2-f983848dcdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REPLAY_BUFFER_CAPACITY = hyp.replay_buffer_size\n",
    "\n",
    "def predict_activity(smiles: str) -> dict:\n",
    "    \"\"\"\n",
    "    Предсказывает pIC50 и IC50 для молекулы по SMILES-строке.\n",
    "    \n",
    "    Аргументы:\n",
    "        smiles (str): SMILES-представление молекулы\n",
    "        \n",
    "    Возвращает:\n",
    "        dict: Словарь с предсказаниями pIC50 и IC50\n",
    "        или сообщение об ошибке\n",
    "    \"\"\"\n",
    "    MODEL = None\n",
    "    COLUMNS = None\n",
    "    \n",
    "    # Загрузка модели и списка колонок при первом вызове\n",
    "    if MODEL is None:\n",
    "        try:\n",
    "            with open('./submodel/final_catboost_model.pkl', 'rb') as f:\n",
    "                MODEL = pickle.load(f)\n",
    "            with open('./submodel/descriptor_columns.pkl', 'rb') as f:\n",
    "                COLUMNS = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Ошибка загрузки модели: {str(e)}\"}\n",
    "    \n",
    "    # Преобразование SMILES в молекулярный объект\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {\"error\": \"Невалидный SMILES\"}\n",
    "    \n",
    "    try:\n",
    "        # Вычисление обычных дескрипторов\n",
    "        desc_calc = {name: func for name, func in Descriptors.descList}\n",
    "        row = {}\n",
    "        \n",
    "        # Разделение колонок на обычные и MACCS\n",
    "        regular_cols = [col for col in COLUMNS if not col.startswith('maccs_')]\n",
    "        maccs_cols = [col for col in COLUMNS if col.startswith('maccs_')]\n",
    "        \n",
    "        # Вычисление химических дескрипторов\n",
    "        for col in regular_cols:\n",
    "            if col in desc_calc:\n",
    "                row[col] = desc_calc[col](mol)\n",
    "            else:\n",
    "                return {\"error\": f\"Неизвестный дескриптор: {col}\"}\n",
    "        \n",
    "        # Генерация MACCS-фингерпринтов\n",
    "        fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        for col in maccs_cols:\n",
    "            bit_idx = int(col.split('_')[1])\n",
    "            row[col] = 1 if fp.GetBit(bit_idx) else 0\n",
    "        \n",
    "\n",
    "        # Создание DataFrame с сохранением порядка колонок\n",
    "        input_data = pd.DataFrame([row], columns=COLUMNS)\n",
    "        \n",
    "        # Предсказание pIC50\n",
    "        pIC50 = MODEL.predict(input_data)[0]# Здесь модель обучалась на pIC50\n",
    "        \n",
    "        # Конвертация в IC50 (в наномолях)\n",
    "        IC50_M = 10 ** (-pIC50)  # в молях\n",
    "        IC50_nM = IC50_M * 1e9   # в наномолях\n",
    "\n",
    "        # Расчет AlogP\n",
    "        alogp = MolLogP(mol)\n",
    "        \n",
    "        return {\n",
    "            \"pIC50\": round(pIC50, 6),\n",
    "            \"IC50\": IC50_nM,\n",
    "            \"AlogP\": round(alogp, 6)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Ошибка предсказания: {str(e)}\"}\n",
    "\n",
    "class QEDRewardMolecule(Molecule):\n",
    "    \n",
    "    def __init__(self, discount_factor, **kwargs):\n",
    "        \n",
    "        super(QEDRewardMolecule, self).__init__(**kwargs)\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def _reward(self):\n",
    "        \n",
    "        molecule = Chem.MolFromSmiles(self._state)\n",
    "        if molecule is None:\n",
    "            return 0.0\n",
    "            \n",
    "        activity = predict_activity(self._state)\n",
    "        IC50 = float(activity['IC50'])\n",
    "        AlogP = float(activity['AlogP'])\n",
    "        qed = QED.qed(molecule)\n",
    "\n",
    "        if IC50 > 200:\n",
    "            reward = -IC50\n",
    "        else:\n",
    "            reward = (100/IC50 + AlogP/6 + qed) * self.discount_factor \n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    def _goal_reached(self):\n",
    "        molecule = Chem.MolFromSmiles(self._state)\n",
    "        if molecule is None:\n",
    "            return 0.0\n",
    "\n",
    "        activity = predict_activity(self._state)\n",
    "        IC50 = float(activity['IC50'])\n",
    "        AlogP = float(activity['AlogP'])\n",
    "        qed = QED.qed(molecule)\n",
    "\n",
    "        return AlogP > 3 and IC50 < 30 and qed > 0.5 and self._counter >= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24537584-e4f3-41a2-988b-929a271dbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    " class RainbowAgent:\n",
    "    def __init__(self, input_length, output_length, device, atoms=51, v_min=-10, v_max=10):\n",
    "        self.device = device\n",
    "        self.atoms = atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.delta_z = (v_max - v_min) / (atoms - 1)\n",
    "        self.support = torch.linspace(v_min, v_max, atoms).to(device)\n",
    "        \n",
    "        # Main and target networks\n",
    "        self.dqn = RainbowDQN(input_length, output_length, atoms, v_min, v_max).to(device)\n",
    "        self.target_dqn = RainbowDQN(input_length, output_length, atoms, v_min, v_max).to(device)\n",
    "        self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "        \n",
    "        self.replay_buffer = PrioritizedReplayBuffer(hyp.replay_buffer_size)\n",
    "        self.optimizer = getattr(opt, hyp.optimizer)(self.dqn.parameters(), lr=hyp.learning_rate)\n",
    "        self.times_of_update = 0\n",
    "    \n",
    "    def get_action(self, observations, epsilon_threshold):\n",
    "        if np.random.uniform() < epsilon_threshold:\n",
    "            return np.random.randint(0, observations.shape[0])\n",
    "        \n",
    "        observations = observations.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.dqn.get_q_values(observations).cpu()\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def update_params(self, batch_size, gamma, polyak):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample from prioritized replay buffer\n",
    "        samples, indices, weights = self.replay_buffer.sample(batch_size)\n",
    "        weights = weights.to(self.device)\n",
    "        \n",
    "        # Unpack batch\n",
    "        states, _, rewards, next_states, dones = zip(*samples)\n",
    "        states = torch.stack([torch.FloatTensor(s) for s in states]).to(self.device)\n",
    "        next_states = torch.stack([torch.FloatTensor(ns) for ns in next_states]).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Distributional DQN update\n",
    "        with torch.no_grad():\n",
    "            # Next state distribution\n",
    "            next_dist = self.target_dqn(next_states)\n",
    "            next_q = (next_dist * self.support).sum(2)\n",
    "            next_actions = next_q.argmax(1)\n",
    "            \n",
    "            # Project next distribution\n",
    "            next_dist = next_dist[range(batch_size), next_actions]\n",
    "            rewards = rewards.unsqueeze(1).expand_as(next_dist)\n",
    "            dones = dones.unsqueeze(1).expand_as(next_dist)\n",
    "            support = self.support.unsqueeze(0).expand_as(next_dist)\n",
    "            \n",
    "            Tz = rewards + gamma * support * (1 - dones)\n",
    "            Tz = Tz.clamp(self.v_min, self.v_max)\n",
    "            b = (Tz - self.v_min) / self.delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "            \n",
    "            offset = torch.linspace(0, (batch_size - 1) * self.atoms, batch_size).long()\\\n",
    "                .unsqueeze(1).expand(batch_size, self.atoms).to(self.device)\n",
    "            \n",
    "            proj_dist = torch.zeros(next_dist.size()).to(self.device)\n",
    "            proj_dist.view(-1).index_add_(0, (l + offset).view(-1), \n",
    "                                          (next_dist * (u.float() - b)).view(-1))\n",
    "            proj_dist.view(-1).index_add_(0, (u + offset).view(-1), \n",
    "                                          (next_dist * (b - l.float())).view(-1))\n",
    "\n",
    "        \n",
    "        # Current state distribution\n",
    "        dist = self.dqn(states)\n",
    "        actions = torch.argmax(self.dqn.get_q_values(states), dim=1)\n",
    "        dist = dist[range(batch_size), actions]\n",
    "        \n",
    "        # Calculate loss\n",
    "        log_dist = torch.log(dist.clamp(min=1e-5))\n",
    "        loss = - (proj_dist * log_dist).sum(1)\n",
    "        weighted_loss = (weights * loss).mean()\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.dqn.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update priorities\n",
    "        priorities = loss.detach().cpu().numpy() + 1e-5\n",
    "        self.replay_buffer.update_priorities(indices, priorities)\n",
    "        \n",
    "        # Update target network\n",
    "        if self.times_of_update % hyp.update_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                for param, target_param in zip(self.dqn.parameters(), self.target_dqn.parameters()):\n",
    "                    target_param.data.copy_(polyak * target_param.data + (1 - polyak) * param.data)\n",
    "        \n",
    "        self.times_of_update += 1\n",
    "        self.dqn.reset_noise()\n",
    "        self.target_dqn.reset_noise()\n",
    "        \n",
    "        return weighted_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc540ae-21d6-443f-bdf5-27bc5bbd6b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Инициализация\n",
    "TENSORBOARD_LOG = True\n",
    "TB_LOG_PATH = \"./runs/dqn/run2\"\n",
    "episodes = 0\n",
    "iterations = 10000\n",
    "num_updates_per_it = 1\n",
    "\n",
    "initmols = pd.read_csv(\"./InitMols.csv\", sep=';')[\"Smiles\"].to_numpy()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "environment = QEDRewardMolecule(\n",
    "    discount_factor=hyp.discount_factor,\n",
    "    atom_types=set(hyp.atom_types),\n",
    "    init_mols = initmols,\n",
    "    allow_removal=hyp.allow_removal,\n",
    "    allow_no_modification=hyp.allow_no_modification,\n",
    "    allow_bonds_between_rings=hyp.allow_bonds_between_rings,\n",
    "    allowed_ring_sizes=set(hyp.allowed_ring_sizes),\n",
    "    max_steps=hyp.max_steps_per_episode,\n",
    ")\n",
    "\n",
    "# Rainbow Agent вместо DQN\n",
    "agent = RainbowAgent(\n",
    "    input_length=hyp.fingerprint_length + 1,\n",
    "    output_length=1,\n",
    "    device=device,\n",
    "    atoms=51,\n",
    "    v_min=-10,\n",
    "    v_max=10\n",
    ")\n",
    "\n",
    "# Загрузка весов\n",
    "# agent.dqn.load_state_dict(torch.load(\"best_weights.pt\"))\n",
    "\n",
    "if TENSORBOARD_LOG:\n",
    "    writer = SummaryWriter(TB_LOG_PATH)\n",
    "\n",
    "environment.initialize()\n",
    "eps_threshold = 1\n",
    "best_reward = -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a99e0b-b677-455d-8a3d-6f1f486e720a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                              | 29/10000 [00:02<19:39,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 0.76, Eps: 0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                              | 30/10000 [00:03<37:22,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QED: 0.124, IC50: 177.815,  AlogP: 0.495, Molecule: [H]C(N)c1c(N(C)C#N)nc2[nH]cc(N=O)c2c1-c1cc2c3c(n1)C(CN)(CN)C2(N=N)NC3=O\n",
      "Saved best model with reward: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▌                                                                           | 330/10000 [00:52<17:43,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Reward: -317.14, Eps: 0.769\n",
      "  QED: 0.268, IC50: 317.143,  AlogP: 0.814, Molecule: Cc1nc2[nH]nc(N3OC4(C5=NN5)C(=O)C(=C=O)C45C(=N)C35)c2c(-c2occ3c(=O)c23)c1Br\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▊                                                                       | 630/10000 [01:41<1:01:41,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20, Reward: 1.04, Eps: 0.738\n",
      "  QED: 0.032, IC50: 150.904,  AlogP: 2.152, Molecule: C=C(NC(O)(Oc1nc(-c2cc3ncc2ON3C(=O)C(O)C2N=C2C)sc1C(=C)N)c1c2n3nnc1c3CC2=Nc1c2c3nc4c(O)c(cc(c14)C2C)C31OC1=N)OO\n",
      "Saved best model with reward: 1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▎                                                                      | 931/10000 [02:43<31:35,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30, Reward: -1172.94, Eps: 0.709\n",
      "  QED: 0.130, IC50: 1172.938,  AlogP: 4.912, Molecule: Cc1c2cc3c(c1-3)N1C(C(O)Oc3c4[nH]c5cc(N=N)cc(cc4ON)c35)c3c4c(F)c5c-4c3C21ON5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▍                                                                   | 1230/10000 [03:43<46:52,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40, Reward: 1.31, Eps: 0.681\n",
      "  QED: 0.120, IC50: 118.251,  AlogP: 2.131, Molecule: CC(N)C12C3=CC1C1C(Oc4nc(-c5c(O)cnc(N6NN(C=N)C7C=C7C6=O)c5O)sc4C(N)=O)=C2N3C1(C)c1cc2c(F)c-2c1\n",
      "Saved best model with reward: 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▊                                                                 | 1530/10000 [04:43<30:38,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Reward: -366.87, Eps: 0.655\n",
      "  QED: 0.115, IC50: 366.873,  AlogP: 0.984, Molecule: [CH]C(=O)C1N=C(OC#CO)CC12C(=C)N2c1nc2c3c4nn2c(N=Cc2c5c6c([n+]([O-])c2-5)C(C)=C6)c1N4CC3(C#N)CN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████                                                               | 1830/10000 [05:46<30:47,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60, Reward: 1.49, Eps: 0.629\n",
      "  QED: 0.099, IC50: 86.316,  AlogP: 1.475, Molecule: N#Cc1nn2nc3c4c5c6c(F)c7ooc8nc(N9C%10=C(C=NN9CN)C9=CC%11=C%10C9%11OO)nc(c8c74)c3c2c5c1C6(N)N\n",
      "Saved best model with reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▎                                                             | 1994/10000 [06:21<27:45,  4.81it/s]"
     ]
    }
   ],
   "source": [
    "for it in tqdm(range(iterations)):\n",
    "    steps_left = hyp.max_steps_per_episode - environment.num_steps_taken\n",
    "    valid_actions = list(environment.get_valid_actions())\n",
    "    \n",
    "    # Подготовка наблюдений\n",
    "    observations = np.vstack([\n",
    "        np.append(\n",
    "            utils.get_fingerprint(act, hyp.fingerprint_length, hyp.fingerprint_radius),\n",
    "            steps_left\n",
    "        ) for act in valid_actions\n",
    "    ])\n",
    "    \n",
    "    observations_tensor = torch.Tensor(observations)\n",
    "    \n",
    "    # Выбор действия\n",
    "    action_idx = agent.get_action(observations_tensor, max(0.1, eps_threshold))\n",
    "    action = valid_actions[action_idx]\n",
    "    \n",
    "    # Шаг среды\n",
    "    result = environment.step(action)\n",
    "    next_state, reward, done = result\n",
    "    \n",
    "    # Сохранение в буфер (с n-step)\n",
    "    action_fingerprint = np.append(\n",
    "        utils.get_fingerprint(action, hyp.fingerprint_length, hyp.fingerprint_radius),\n",
    "        steps_left\n",
    "    )\n",
    "\n",
    "    steps_left_next = steps_left - 1 if not done else 0\n",
    "    next_state_fp = np.append(\n",
    "        utils.get_fingerprint(action, hyp.fingerprint_length, hyp.fingerprint_radius),\n",
    "        steps_left_next\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Добавляем переход в буфер \n",
    "    agent.replay_buffer.add((\n",
    "        action_fingerprint, \n",
    "        action_idx, \n",
    "        reward, \n",
    "        next_state_fp, \n",
    "        float(done)\n",
    "    ), n_step=3, gamma=hyp.discount_factor)\n",
    "    \n",
    "    # Обновление модели\n",
    "    if it % hyp.update_interval == 0 and len(agent.replay_buffer) > hyp.batch_size:\n",
    "        loss = agent.update_params(hyp.batch_size, hyp.gamma, hyp.polyak)\n",
    "        \n",
    "        if TENSORBOARD_LOG and loss is not None:\n",
    "            writer.add_scalar(\"training/loss\", loss, it)\n",
    "    \n",
    "    # Обработка завершения эпизода\n",
    "    if done:\n",
    "        final_reward = reward\n",
    "        \n",
    "        if TENSORBOARD_LOG:\n",
    "            writer.add_scalar(\"episode/reward\", final_reward, episodes)\n",
    "            writer.add_scalar(\"episode/epsilon\", eps_threshold, episodes)\n",
    "        \n",
    "        # Логирование и сохранение лучшей модели\n",
    "        if episodes % 10 == 0:\n",
    "            print(f\"Episode {episodes}, Reward: {final_reward:.2f}, Best Reward: {best_reward:.2f}, Eps: {eps_threshold:.3f}\")\n",
    "            mol = Chem.MolFromSmiles(environment._state)\n",
    "            if mol:\n",
    "                activity = predict_activity(environment._state)\n",
    "                IC50 = float(activity['IC50'])\n",
    "                AlogP = float(activity['AlogP'])\n",
    "                qed = QED.qed(mol)\n",
    "\n",
    "                sa = sascorer.calculateScore(mol)\n",
    "                print(f\"  QED: {qed:.3f}, IC50: {IC50:.3f},  AlogP: {AlogP:.3f}, Molecule: {environment._state}\")\n",
    "                \n",
    "        if final_reward > best_reward:\n",
    "            torch.save(agent.dqn.state_dict(), 'best_weights.pt')\n",
    "            print(f\"Saved best model with reward: {final_reward:.2f}\")\n",
    "            best_reward = final_reward\n",
    "        \n",
    "        # Сброс среды и уменьшение epsilon\n",
    "        environment.initialize()\n",
    "        eps_threshold = max(hyp.epsilon_end, eps_threshold * hyp.gamma)\n",
    "        episodes += 1\n",
    "\n",
    "# Закрытие логгера\n",
    "if TENSORBOARD_LOG:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9071e5-6b23-43a6-b35c-089a43e1d112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generated_molecules = []\n",
    "num_molecules_to_generate = 100\n",
    "agent.dqn.eval()\n",
    "eps_threshold = 0.03\n",
    "\n",
    "for it in range(num_molecules_to_generate):\n",
    "    done = False\n",
    "    environment.initialize()\n",
    "    while not done:\n",
    "        steps_left = hyp.max_steps_per_episode - environment.num_steps_taken\n",
    "        valid_actions = list(environment.get_valid_actions())\n",
    "    \n",
    "        observations = np.vstack(\n",
    "            [\n",
    "                np.append(\n",
    "                    utils.get_fingerprint(\n",
    "                        act, hyp.fingerprint_length, hyp.fingerprint_radius\n",
    "                    ),\n",
    "                    steps_left,\n",
    "                )\n",
    "                for act in valid_actions\n",
    "            ]\n",
    "        ) \n",
    "    \n",
    "        observations_tensor = torch.Tensor(observations)\n",
    "        a = agent.get_action(observations_tensor, eps_threshold)\n",
    "        action = valid_actions[a]\n",
    "        result = environment.step(action)\n",
    "    \n",
    "        action_fingerprint = np.append(\n",
    "            utils.get_fingerprint(action, hyp.fingerprint_length, hyp.fingerprint_radius),\n",
    "            steps_left,\n",
    "        )\n",
    "    \n",
    "        next_state, reward, done = result\n",
    "        steps_left = hyp.max_steps_per_episode - environment.num_steps_taken\n",
    "    \n",
    "        next_state = utils.get_fingerprint(\n",
    "            next_state, hyp.fingerprint_length, hyp.fingerprint_radius\n",
    "        )  \n",
    "    \n",
    "        action_fingerprints = np.vstack(\n",
    "            [\n",
    "                np.append(\n",
    "                    utils.get_fingerprint(\n",
    "                        act, hyp.fingerprint_length, hyp.fingerprint_radius\n",
    "                    ),\n",
    "                    steps_left,\n",
    "                )\n",
    "                for act in environment.get_valid_actions()\n",
    "            ]\n",
    "        )\n",
    "        #print(environment._state)\n",
    "        print(reward)\n",
    "        \n",
    "    generated_molecules.append(environment._state)\n",
    "    print(generated_molecules[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcda02-d503-45c1-9cd8-94caa19f7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e69c97-e936-4e99-80c6-184ec4bc62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "valid_smiles = []\n",
    "for smi in generated_molecules:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        valid_smiles.append(smi)\n",
    "\n",
    "print(f\"Сгенерировано валидных молекул: {len(valid_smiles)}/{len(generated_molecules)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
